{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **My Personal Language Model**\n",
    "\n",
    "_Developed by: Paolo Edni Andryn V. Espiritu_\n",
    "<br>\n",
    "_Course: CSC714M | Theories In Natural Language Processing_\n",
    "\n",
    "In this project, I will be making a **probabilistic language model** using a dataset that contains my messenger conversation data from 2021-2024. The objective is to make a model that captures the way how I speak to people (e.g. peers, friends, family, and even strangers).\n",
    "\n",
    "As suggested in class, I will be using **Jadesse Chan's** statistical trigram language model found in this link: https://github.com/jadessechan/Text-Prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The first step involves preparing the data that will be provided to the language model. With this, I created a scraper that will retrieve the user's messages given the `directory name of the downloaded messenger folder` and the `sender's name`. The `config` file used in the scraper contains some texts that I specifically excluded. You may also create your own config file for your specific case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "\n",
    "path = \"./messenger_data\"\n",
    "sender_name = \"Paolo Espiritu\"\n",
    "\n",
    "helper.scrape_messenger_data(path, sender_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also check the size of the corpus in terms of the number of tokens and its vocabulary. For this, we will use the **word_tokenize** function from the **nltk** library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./dataset.txt\"\n",
    "tokens = []\n",
    "\n",
    "with open(dataset_path) as input_file:\n",
    "    tokens = word_tokenize(input_file.read())\n",
    "\n",
    "print(\"tokens: {} | vocabulary size: {}\".format(len(tokens), len(set(tokens))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Trigram language model\n",
    "\n",
    "Now that we have generated the `dataset.txt` file, we can now proceed to use Jadesse Chan's statistical trigram language model. In this section, we can generate a sentence based on the given phrase of the user. Ideally, the model should sound like the person whose dataset was used in this project.\n",
    "\n",
    "In my case, I need to come up with specific phrases that I often use to know if the output sounds like something I would say.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Install NLTK resources\n",
    "\"\"\"\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Normalize text, remove unnecessary characters, \n",
    "    perform regex parsing, and make lowercase\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def filter(text):\n",
    "    # normalize text\n",
    "    text = (\n",
    "        unicodedata.normalize(\"NFKD\", text)\n",
    "        .encode(\"ascii\", \"ignore\")\n",
    "        .decode(\"utf-8\", \"ignore\")\n",
    "    )\n",
    "    # replace html chars with ' '\n",
    "    text = re.sub(\"<.*?>\", \" \", text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(\" \", \" \", string.punctuation))\n",
    "    # only alphabets and numerics\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # replace newline with space\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # split and join the words\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Tokenize remaining words\n",
    "    and perform lemmatization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    output = []\n",
    "    for words in tokens:\n",
    "        # lemmatize words\n",
    "        output.append(wnl.lemmatize(words))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Make a language model using a dictionary, trigrams, \n",
    "    and calculate word probabilities\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def n_gram_model(text):\n",
    "    trigrams = list(\n",
    "        nltk.ngrams(\n",
    "            text,\n",
    "            3,\n",
    "            pad_left=True,\n",
    "            pad_right=True,\n",
    "            left_pad_symbol=\"<s>\",\n",
    "            right_pad_symbol=\"</s>\",\n",
    "        )\n",
    "    )\n",
    "    bigrams = list(\n",
    "        nltk.ngrams(\n",
    "            text,\n",
    "            2,\n",
    "            pad_left=True,\n",
    "            pad_right=True,\n",
    "            left_pad_symbol=\"<s>\",\n",
    "            right_pad_symbol=\"</s>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # N-gram Statistics\n",
    "    # get freq dist of trigrams\n",
    "    # freq_tri = nltk.FreqDist(trigrams)\n",
    "    freq_bi = nltk.FreqDist(bigrams)\n",
    "    # freq_tri.plot(30, cumulative=False)\n",
    "    freq_bi.plot(30, cumulative=False)\n",
    "    # print(\"Most common trigrams: \", freq_tri.most_common(5))\n",
    "    print(\"Most common bigrams: \", freq_bi.most_common(5))\n",
    "\n",
    "    # make conditional frequencies dictionary\n",
    "    cfdist = ConditionalFreqDist()\n",
    "\n",
    "    for w1, w2 in bigrams:\n",
    "        cfdist[w1][w2] += 1\n",
    "    # transform frequencies to probabilities\n",
    "    for w1 in cfdist:\n",
    "        total_count = float(sum(cfdist[w1].values()))\n",
    "        for w2 in cfdist[w1]:\n",
    "            cfdist[w1][w2] /= total_count\n",
    "\n",
    "    return cfdist\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Generate predictions from the Conditional Frequency Distribution\n",
    "    dictionary (param: model), append weighted random choice to user's phrase,\n",
    "    allow option to generate more words following the prediction\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def predict(model, user_input):\n",
    "    user_input = filter(user_input)\n",
    "    user_input = user_input.split()\n",
    "\n",
    "    # w1 = len(user_input) - 2\n",
    "    # w2 = len(user_input) - 1\n",
    "    # prev_words = user_input[w1 : w2 + 1]\n",
    "\n",
    "    # # display prediction from highest to lowest maximum likelihood\n",
    "    # prediction = sorted(\n",
    "    #     dict(model[prev_words[0], prev_words[1]]),\n",
    "    #     key=lambda x: dict(model[prev_words[0], prev_words[1]])[x],\n",
    "    #     reverse=True,\n",
    "    # )\n",
    "    # Get the last word from the user input\n",
    "    prev_word = (\n",
    "        user_input[-1] if user_input else \"<s>\"\n",
    "    )  # Use '<s>' if user_input is empty\n",
    "\n",
    "    # Display predictions from highest to lowest maximum likelihood\n",
    "    prediction = sorted(\n",
    "        model[prev_word], key=lambda x: model[prev_word][x], reverse=True\n",
    "    )\n",
    "\n",
    "    print(\"Trigram model predictions: \", prediction)\n",
    "\n",
    "    word = []\n",
    "    weight = []\n",
    "    for key, prob in dict(model[prev_words[0], prev_words[1]]).items():\n",
    "        word.append(key)\n",
    "        weight.append(prob)\n",
    "\n",
    "    # pick from a weighted random probability of predictions\n",
    "    next_word = random.choices(word, weights=weight, k=1)\n",
    "    # add predicted word to user input\n",
    "    user_input.append(next_word[0])\n",
    "    print(\" \".join(user_input))\n",
    "\n",
    "    ask = input(\n",
    "        \"Do you want to generate another word? (type 'y' for yes or 'n' for no): \"\n",
    "    )\n",
    "    if ask.lower() == \"y\":\n",
    "        predict(model, str(user_input))\n",
    "    elif ask.lower() == \"n\":\n",
    "        print(\"done\")\n",
    "\n",
    "\n",
    "def use_model(path):\n",
    "    file = open(path, \"r\")\n",
    "    text = \"\"\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        text += line\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "    # pre-process text\n",
    "    print(\"Filtering...\")\n",
    "    words = filter(text)\n",
    "    print(\"Cleaning...\")\n",
    "    words = clean(words)\n",
    "\n",
    "    # make language model\n",
    "    print(\"Making model...\")\n",
    "    model = n_gram_model(words)\n",
    "\n",
    "    print(\"Enter a phrase: \")\n",
    "    user_input = input()\n",
    "    predict(model, user_input)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"./dataset.txt\"\n",
    "\n",
    "    use_model(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
